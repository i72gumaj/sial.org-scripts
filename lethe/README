Lethe - Data Begins to Forget Itself 		 
	  	
file(1) and similar software provide for efficient identification of 		 
various file formats via "magic" tests. Obscuring this information may 		 
be of some value, for example to hide file contents from a quick  	 
filesystem scan--assuming that the file extension, if any, is also masked--    
or to slow down analysis of the thus unknown file contents. Obviously, 		 
the code and especially the specific steps taken to obscure the data			 
must be kept secure, as unless the goal is merely file corruption, 		 
there must be a way to reverse the obfuscations, and if those the data				
is not for can trivially undo the lethery, then there is no point to 			
this exercise. 	  
  	 
Binary files are relatively easy to mask, as very little must be changed    
to throw off a magic test. Plain text may require more work, especially 			
if the text must be masked from casual human observation. ASCII armored  		
PGP messages have very distinctive header and footers, for example. 		 
Modest obfuscations might easily be detectable via suitable rules added 	 	
to an intrusion detection or other data auditing system, especially if 		 
particular obfuscation steps remain in use over long periods of time. 	 	
 		 
Plain text might profitably be converted to word salad spam, or encoded			 
into a JPEG, with that file having proper headers and such: only if a  	 
human attempted to view the image, or deep checks performed on the file    
contents, would the chameleon be revealed, depending on how seamlessly 			
the non-JPEG data is melded to the JPEG format. The point of this 	  
exercise: deny the simple and efficient categorization of data, require 		 
that more time and energy be spent on analysis, or force the risk of not	   
learning what the thus obscured data is. 		 
 	 	
A downside: relatively few people will carry out such modifications, so 		 
if the modifications are detected, they may stand out like both PGP			 
users do in a sea of unencrypted communication. This may attract  	 
unwanted attention to the data and its milieu, or provide an excellent    
means to divert limited manpower and resources towards a false scent: a 		 
data nerd flytrap, perhaps. Also, the need for the method to remain			 
secure limits the audience; an individual or small set of individuals 		 
might mask their data or communications this way, but the obfuscations 	 	
will not scale to widespread uses, unless standardized methods of 			
sufficient CPU costs can be devised to better deny real-time scanning 		 
and categorization of file contents. Partial obfuscation may suffice; 		 
for example, it may be known that a file is a MP3, via a .mp3 extension, 	 	
but other details of the file contents obscured (length, ID tags, enough 			
obfuscation to cause a MP3 parser to fail to read the data), and only  	 
with specific steps could the proper contents be recovered.  	 
    
Why not just encrypt? Because encryption may stand out like a sore 		 
thumb, and there may be standard operating procedures for it as there 		 
are guides to Western chess openings. Obfuscating the data should thwart 		 
automated data classification and analysis, and force either the risk of				
ignoring the obfuscated data--"it's just spam"--or the costs of bringing 			
more expensive resources to bear on the unknown--"I think there's  	 
something here, just give me a few more hours to work on the problem." 		 
Standard encryption could help with the known MP3 case; a random 			
password might be used, forcing anyone who wants to recover the audio to 		 
brute force that password. Whether to encrypt, obfuscate, or both will				
depend on the data, its uses, and other goals. 			
 	  
On Deliberate Data Corruption 			
 	  
Random file corruption may also be a goal; in this case, data near the 		 
beginning of the file should remain unharmed, so that the header remains 	 	
correct, and data elsewhere altered. If the file type is known, care may 		 
be taken to alter the meat and not the skeleton of the format, or to			 
systemically alter the data: change the numbers and thus their results
or relationships, introduce spelling or grammar errors, or the like. The
attacker should thus have knowledge of various common file formats, some
idea of how to change the data, and ideally some statistical chops--
introducing corruptions that can then be ignored as outliers would make
for poor work.

Checksums or version control would help detect such changes; therefore,
care must be taken in identifying proper targets for this attack: small
data, where the user has not bothered with data control, or large data,
where there is too much information to perform checksums on, or if the
attack is made while results are being generated, so that the corruption
is lost in the routine of workflow. These would likely require insider
knowledge to pull off properly.

Ruminations On Periodic Changes

Timing and other metadata may be masked by automation; if one reads a website
say in the morning, and again over lunch, this could instead be automated by
saving a copy say four times per day, and reading the cached copy. This would
eliminate the timing information of the website reads, though might still
record client laptop to resource cache interactions, assuming a mostly
offline laptop.

Commits could also be masked by this method, e.g. by staging `git commit` with
at(1) jobs, though the complexity of that may preclude effective use; on the
other hand, public commits and pushes should not be too hard to batch up into
automated jobs. This would mask the actual commit times and related metadata
available from that, down to the frequency with which such public commits are
made, though if all such commits are made say, daily, or weekly, that should
greatly mask when the changes were actually made.

Timing on comments or other posts to websites would be difficult to mask, as
these would require automation of the various particular websites, etc. Mail
might be eaiser, e.g. with a MTA that only releases batched mail at periodic
intervals (say, once an hour), though these communication delays might pose
difficulties in conducting online communications, e.g. for a new job. Such slow
deliveries would help mask when the actual mail was composed, lowering the
timing and correlation resolution of anyone listening for such events.

On the other hand, increasing the noise, by making web requests more frequent,
would also help mask timings, and increase the data collection requirements of
listeners for such metadata. This would work for pull requests, though less so
for push comments on websites or sending of mail. Mail could send a message of
the same size at the same times each day, filled with (encrypted) gibberish or
the actual (encrypted) message, if there is one, to thwart listeners for when
messages are sent. This would force each actual message to be looked at and
decryption or other techniques used to try to figure out whether the message is
gibberish or not. The (encrypted) gibberish would ideally need to be plain-text
similar to regularly sent messages, e.g. generated via a large corpus and
markov engine, and the actual messages perhaps also mostly gibberish.

Largly disconnected laptops will be problematic, as these will pop on and off
the Internet, and need to communicate with cache locations, activity very easy
for someone interested in to look for. Therefore, the ideal communications
point would be something always on the Internet, and non-Internet methods used
to get data to and from that batching/periodic communication node.

Most of the population will not take such measures, so taking such measures
as are outlined above might well attract notice.
